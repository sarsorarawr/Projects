{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLM implementation",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#FIXED WINDOW NEURO LANGUAGE MODEL\n",
        "\n",
        "*  prefix size = 2 meaning take first two words and predict last\n",
        "*   we have to tokenize (cut them into substrings) our input and make them into indices numbers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fhv-bFJgww6K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7tv9eEhvnIp"
      },
      "outputs": [],
      "source": [
        "sentences = ['thanh likes pickles','she always naps', 'victoria is tired','beba likes boba']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {} #maps word type to index\n",
        "inputs = [] #stores an indexified version of each sentence\n",
        " \n",
        "for sent in sentences:\n",
        "  sent_indx = []\n",
        "  sent = sent.split() #tokenizing the sentences\n",
        "  for word in sent:\n",
        "    if word not in vocab:\n",
        "      vocab[word] = len(vocab) #add new unique type of index to vocab\n",
        "    sent_indx.append(vocab[word]) #labeling each word to an index in the sentence\n",
        "  inputs.append(sent_indx)\n",
        "\n",
        "print(vocab)\n",
        "print(inputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru-uPtxGwv7C",
        "outputId": "5bb836fa-983d-4aa3-a097-8a7c174e5335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'thanh': 0, 'likes': 1, 'pickles': 2, 'she': 3, 'always': 4, 'naps': 5, 'victoria': 6, 'is': 7, 'tired': 8, 'beba': 9, 'boba': 10}\n",
            "[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 1, 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n"
      ],
      "metadata": {
        "id": "EY8iOCDFzGiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. convert to LongTensors\n",
        "2. define inputs and outputs\n",
        "(given first two words predict third)\n",
        "\n"
      ],
      "metadata": {
        "id": "OLiArIU5znSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefixes = torch.LongTensor([sent[:-1] for sent in inputs]) #prefixes first two words \n",
        "labels = torch.LongTensor([sent[-1] for sent in inputs]) #last word, defining inputs and outputs"
      ],
      "metadata": {
        "id": "uy5iz3QGzxNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Onto defining the network:\n"
      ],
      "metadata": {
        "id": "zg8ZC6Mj0wwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NLM(nn.Module):\n",
        "  #write init function (initializes all the parameters of the networks)\n",
        "  #forward function (defines the forward propagation computations)\n",
        "\n",
        "  def __init__(self, d_embedding, d_hidden, window_size, len_vocab):\n",
        "    super(NLM, self).__init__() #init the base Module class\n",
        "    self.embeddings = nn.Embedding(len_vocab, d_embedding)\n",
        "    self.d_embs = d_embedding\n",
        "    self.W_hid = nn.Linear(d_embedding*window_size,d_hidden) #concatenated embeddings to hidden\n",
        "    self.W_out = nn.Linear(d_hidden, len_vocab) #hidden to output probability distribution over vocab\n",
        "\n",
        "  def forward(self, input): #each input will be a batch of prefixes (for our example is 4)\n",
        "    batch_size, window_size = input.size() #dimensionality of input, batchsize= 4 sentences, prefix = first 2 words\n",
        "    embs = self.embeddings(input) #dimension:  4 x 2 x 5\n",
        "    # print('embedding size:', embs.size())\n",
        "\n",
        "    #next, we want to concatenate the prefix embeddings together\n",
        "    concat_embs = embs.view(batch_size, window_size * self.d_embs)\n",
        "    # print('concatenated embs size:', concat_embs.size())\n",
        "\n",
        "    #print(embs[0])\n",
        "    #print(concat_embs[0])\n",
        "\n",
        "    #now, we project this to the hidden space\n",
        "    hiddens = self.W_hid(concat_embs)\n",
        "    # print('hidden size:', hiddens.size())\n",
        "\n",
        "    #finally, project hiddens to vocabulary space\n",
        "    outs = self.W_out(hiddens)\n",
        "    # print('output size:', outs.size()) #unique vocab size is 11\n",
        "\n",
        "    return outs #return unnormalized probability (logits)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "network = NLM(d_embedding=5,d_hidden=12,window_size=2, len_vocab=len(vocab))\n",
        "\n",
        "num_epochs= 20 #how many times we are going to go through our entire training dataset\n",
        "learning_rate = 0.1 # modify this if training isn't converging, etc.\n",
        "loss_fn = nn.CrossEntropyLoss() #average cross entropy loss over each prefix in batch\n",
        "optimizer = torch.optim.SGD(params=network.parameters(),lr=learning_rate) \n",
        "\n",
        "#training loop\n",
        "for i in range(num_epochs):\n",
        "  logits = network(prefixes)\n",
        "  loss = loss_fn(logits, labels)\n",
        "  \n",
        "  #update our params to make the loss smaller\n",
        "  #1. compute gradient (partial derivs of loss with respect to parameters)\n",
        "  loss.backward()\n",
        "\n",
        "  #2. update params using gradient descent\n",
        "  optimizer.step()\n",
        "\n",
        "  #3. zero out the gradients for the next epoch\n",
        "  optimizer.zero_grad()\n",
        "  print('epoch: %d. loss %0.2f' % (i,loss))"
      ],
      "metadata": {
        "id": "mhozoKlz00gp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "637d023f-eae3-4398-b0e4-6beff9c72928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0. loss 2.72\n",
            "epoch: 1. loss 2.47\n",
            "epoch: 2. loss 2.25\n",
            "epoch: 3. loss 2.04\n",
            "epoch: 4. loss 1.84\n",
            "epoch: 5. loss 1.66\n",
            "epoch: 6. loss 1.48\n",
            "epoch: 7. loss 1.32\n",
            "epoch: 8. loss 1.17\n",
            "epoch: 9. loss 1.03\n",
            "epoch: 10. loss 0.91\n",
            "epoch: 11. loss 0.81\n",
            "epoch: 12. loss 0.72\n",
            "epoch: 13. loss 0.64\n",
            "epoch: 14. loss 0.57\n",
            "epoch: 15. loss 0.51\n",
            "epoch: 16. loss 0.45\n",
            "epoch: 17. loss 0.41\n",
            "epoch: 18. loss 0.37\n",
            "epoch: 19. loss 0.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our loss is decreasing close to zero. But does it predict the right word?"
      ],
      "metadata": {
        "id": "7Bkh7BfBI63N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's define a reverse vocabulary mapping (index to word type)\n",
        "rev_vocab = dict((indx, word) for (word, indx) in vocab.items())\n",
        "\n",
        "thanhlikes = prefixes[0].unsqueeze(0)\n",
        "logits = network(thanhlikes)\n",
        "probs = nn.functional.softmax(logits,dim=1).squeeze()\n",
        "argmax_indx = torch.argmax(probs).item()\n",
        "print('given \"thanh likes\", the model predicts \"%s\" as the next word with %0.4f probability' % (rev_vocab[argmax_indx], probs[argmax_indx]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEjpQP00I41r",
        "outputId": "2211a5fc-52b7-47c0-fa93-48e6137f0b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "given \"thanh likes\", the model predicts \"pickles\" as the next word with 0.7553 probability\n"
          ]
        }
      ]
    }
  ]
}